{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2LMWa0jSh8iieH7bFwqpK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharon504/ml-learning/blob/main/makemore_name_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhBq0Q0Rjula"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEu3E4WOj0i4",
        "outputId": "6e7b6a5b-a700-4962-b241-27d31667b0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-04 06:32:24--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-06-04 06:32:24 (6.27 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(open('names.txt', 'r').read().splitlines())\n",
        "words[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivl07r14j1wd",
        "outputId": "a23b6ae1-8d9e-440d-eaef-ea942d842638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {w: i for i, w in enumerate(['.'] + sorted(list(set(\"\".join(words)))))}\n",
        "itos = {i: w for w, i in stoi.items()}\n",
        "vocab_size = len(stoi)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKJ2XnkTkTL3",
        "outputId": "e1bbf489-3145-45df-e7f9-b9d6e4388d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(words, block_size):\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  for word in words:\n",
        "    context = [0] * block_size\n",
        "    for char in word:\n",
        "      x.append(context)\n",
        "      y.append(stoi[char])\n",
        "      context = context[1:] + [stoi[char]]\n",
        "  x = torch.tensor(x)\n",
        "  y = torch.tensor(y)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "oiv5s93qlko8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(words, train, test, block_size):\n",
        "  import random\n",
        "  random.seed(42)\n",
        "  random.shuffle(words)\n",
        "  n = len(words)\n",
        "\n",
        "  n1 = int(train * n)\n",
        "  n2 = int(test * n) + n1\n",
        "  x_train, y_train = build_dataset(words[:n1], block_size)\n",
        "  x_test, y_test = build_dataset(words[n1:n2], block_size)\n",
        "  x_val, y_val = build_dataset(words[n2:], block_size)\n",
        "  return x_train, x_test, x_val, y_train, y_test, y_val"
      ],
      "metadata": {
        "id": "3p2Na4ZzouT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding:\n",
        "  def __init__(self, vocab_size, emb_dim):\n",
        "    self.weight = torch.randn(vocab_size, emb_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = self.weight[x]\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight]"
      ],
      "metadata": {
        "id": "zKrfJepDb1aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlattenConsecutive:\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "  def __call__(self, x):\n",
        "    a, b, c = x.shape\n",
        "    x = x.view(a, b // self.n, c * self.n)\n",
        "    if x.shape[1] == 1:\n",
        "      x = x.squeeze(dim=1)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "vesuX3iQc-Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out)) / fan_in ** 0.5\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])"
      ],
      "metadata": {
        "id": "oeFtBtcxqvEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm:\n",
        "  def __init__(self, dims, momentum=1e-2, eps=1e-5, training=True):\n",
        "    self.dims = dims\n",
        "    self.momentum = momentum\n",
        "    self.eps = eps\n",
        "    self.training = training\n",
        "\n",
        "    self.bnmean_running = torch.zeros(dims)\n",
        "    self.bnvar_running = torch.ones(dims)\n",
        "\n",
        "    self.gamma = torch.ones(dims)\n",
        "    self.beta = torch.zeros(dims)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if self.training:\n",
        "      if x.ndim <= 2:\n",
        "        dim = 0\n",
        "      else:\n",
        "        dim = tuple(list(range(0, x.ndim - 1)))\n",
        "      bnmean = x.mean(dim, keepdim=True)\n",
        "      bnvar = x.var(dim, keepdim=True)\n",
        "    else:\n",
        "      bnmean = self.bnmean_running\n",
        "      bnvar = self.bnvar_running\n",
        "    hpre = (x - bnmean) / torch.sqrt(bnvar + self.eps)\n",
        "    self.out = self.gamma * hpre + self.beta\n",
        "\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.bnmean_running = self.bnmean_running * (1 - self.momentum) + bnmean * self.momentum\n",
        "        self.bnvar_running = self.bnvar_running * (1 - self.momentum) + bnvar * self.momentum\n",
        "\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "dD6hAQQ_yMS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "2fxOt6Se3jCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential:\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [p for layer in self.layers for p in layer.parameters()]"
      ],
      "metadata": {
        "id": "0QG2x2yge81H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(X, Y, model, epoch=2e5, batch_size=32):\n",
        "  lossi = []\n",
        "  parameters = model.parameters()\n",
        "  print(sum(p.nelement() for p in parameters))\n",
        "  for p in parameters:\n",
        "    p.requires_grad = True\n",
        "  for i in range(int(epoch)):\n",
        "    ix = torch.randint(0, X.shape[0], (batch_size,))\n",
        "    x = X[ix]\n",
        "    y = Y[ix]\n",
        "\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    lr = 1e-1 if i < 1e4 else 1e-2\n",
        "    for p in parameters:\n",
        "      p.data -= p.grad * lr\n",
        "\n",
        "    if i % 10000 == 0:\n",
        "      print(f\"{i}/{epoch} - {loss}\")\n",
        "    lossi.append(loss)\n",
        "  return lossi, parameters"
      ],
      "metadata": {
        "id": "z0ZRLXjU7LD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 24\n",
        "n_hidden = 128\n",
        "flatten_by = 2\n",
        "block_size = 8\n",
        "x_train, x_test, x_val, y_train, y_test, y_val = split_dataset(words, 0.8, 0.1, block_size)\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, emb_dim),\n",
        "    FlattenConsecutive(flatten_by), Linear(emb_dim * flatten_by, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),\n",
        "    FlattenConsecutive(flatten_by), Linear(flatten_by * n_hidden, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),\n",
        "    FlattenConsecutive(flatten_by), Linear(flatten_by * n_hidden, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),\n",
        "    Linear(n_hidden, vocab_size)\n",
        "])\n",
        "\n",
        "with torch.no_grad():\n",
        "  # last layer: make less confident\n",
        "  #layers[-1].weight *= 0.1\n",
        "  # all other layers: apply gain\n",
        "  for layer in model.layers[:-1]:\n",
        "    if isinstance(layer, Linear):\n",
        "      layer.weight *= 5/3\n"
      ],
      "metadata": {
        "id": "6WGBCBqM35hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, params = training(x_train, y_train, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tr5zRof8sgI",
        "outputId": "f4dcd579-7f0e-403a-8971-dfffbb0fc10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76579\n",
            "0/200000.0 - 3.397691488265991\n",
            "10000/200000.0 - 2.3353497982025146\n",
            "20000/200000.0 - 2.4722025394439697\n",
            "30000/200000.0 - 2.259833574295044\n",
            "40000/200000.0 - 2.0992672443389893\n",
            "50000/200000.0 - 2.4642746448516846\n",
            "60000/200000.0 - 1.8219878673553467\n",
            "70000/200000.0 - 2.1999118328094482\n",
            "80000/200000.0 - 1.6512300968170166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "  print(layer.__class__.__name__, ': ', tuple(layer.out.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdh8yLXutjkf",
        "outputId": "23ce28f5-2ffb-4d6b-c90d-68e6c2f63372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding :  (32, 8, 10)\n",
            "FlattenConsecutive :  (32, 4, 20)\n",
            "Linear :  (32, 4, 68)\n",
            "BatchNorm :  (32, 4, 68)\n",
            "Tanh :  (32, 4, 68)\n",
            "FlattenConsecutive :  (32, 2, 136)\n",
            "Linear :  (32, 2, 68)\n",
            "BatchNorm :  (32, 2, 68)\n",
            "Tanh :  (32, 2, 68)\n",
            "FlattenConsecutive :  (32, 136)\n",
            "Linear :  (32, 68)\n",
            "BatchNorm :  (32, 68)\n",
            "Tanh :  (32, 68)\n",
            "Linear :  (32, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.tensor(loss).view(-1, 10000).mean(1))"
      ],
      "metadata": {
        "id": "HVrPBuD_ABy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "  if isinstance(BatchNorm, layer):\n",
        "    layer.training = False"
      ],
      "metadata": {
        "id": "rP-yHzI3AGj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  logits_train = model(x_train)\n",
        "  loss_train = F.cross_entropy(logits_train, y_train)\n",
        "\n",
        "  logits_val = model(x_val)\n",
        "  loss_val = F.cross_entropy(logits_val, y_val)\n",
        "  loss_train, loss_val"
      ],
      "metadata": {
        "id": "toXZ2uUgnC3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6Lc8iAEwV6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}